# 第4章 神经网络的学习

* 学习：从训练数据中自动获取最优权重参数的过程。
* 损失函数
* 函数斜率梯度法

## 4.1 从数据中学习
* 感知机收敛定理
                                                      
### 4.1.1 数据驱动
* 特征量：可以从数据输入中准确地提取本质数据的转换器
* 使用特征量的机器学习
* 端到端的深度学习
* 神经网络优点：所有的问题都可以用同样的流程来解决。

### 4.1.2 训练数据和测试数据
* 训练数据（监督数据）和测试数据
* 泛化能力：处理未被观察过的数据（不包含训练数据）的能力。
* 过拟合（over fitting）：只对某个数据集过度拟合的状态。

## 4.2 损失函数
* 神经网络学习中所用的指标：损失函数（loss function）  
  损失函数表示当前的神经网络对监督数据在多大程度上不拟合
  * 均方误差（mean squared error）
  * 交叉熵误差（cross entropy error）
    
* one-hot表示：正确标签表示为1，其他为0的表示方法
* 

### 4.2.1 均方误差
### 4.2.2 交叉熵误差
### 4.2.3 mini-batch学习
### 4.2.4 mini-batch版交叉熵误差的实现
### 4.2.5 为何要设定损失函数

## 4.3 数值微分
### 4.3.1 导数
### 4.3.2 数值微分的例子
### 4.3.3 偏导数

## 4.4 梯度
### 4.4.1 梯度法
### 4.4.2 神经网络的梯度

## 4.5 学习算法的实现
### 4.5.1 2层神经网络的类
### 4.5.2 mini-batch的实现
### 4.5.3 基于测试数据的评价

## 4.6 小结







